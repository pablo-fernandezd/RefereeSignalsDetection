{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T07:54:28.833951Z",
     "start_time": "2025-03-28T07:52:51.963091Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11x summary (fused): 464 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "üöÄ Starting video processing pipeline...\n",
      "\n",
      "üöÄ Starting processing: videoSplitted.mp4\n",
      "üîç Processing single video chunk...\n",
      "üé¨ Started segment 2 at 0:00:00\n",
      "\n",
      "‚è±Ô∏è Progress [0:01:25] 6.9% complete | Segments: 2 | Recent detections: 515\n",
      "\n",
      "‚è±Ô∏è Progress [0:03:00] 14.5% complete | Segments: 2 | Recent detections: 569\n",
      "üßπ Removed short segment (<300s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 275\u001B[0m\n\u001B[0;32m    273\u001B[0m processor \u001B[38;5;241m=\u001B[39m VideoProcessor()\n\u001B[0;32m    274\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124müöÄ Starting video processing pipeline...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 275\u001B[0m processor\u001B[38;5;241m.\u001B[39mprocess_videos()\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124müéâ All processing completed successfully!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[7], line 48\u001B[0m, in \u001B[0;36mVideoProcessor.process_videos\u001B[1;34m(self, input_dir, output_dir, used_dir)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124müöÄ Starting processing: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvideo_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 48\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_single_video(video_path, output_dir, used_dir)\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m‚úÖ Successfully processed: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvideo_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "Cell \u001B[1;32mIn[7], line 72\u001B[0m, in \u001B[0;36mVideoProcessor._process_single_video\u001B[1;34m(self, video_path, output_dir, used_dir)\u001B[0m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124müîç Processing single video chunk...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 72\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_video_chunk(video_path, output_dir)\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_safe_move(video_path, used_dir)\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124müì¶ Moved original to: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mused_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[7], line 132\u001B[0m, in \u001B[0;36mVideoProcessor._process_video_chunk\u001B[1;34m(self, video_path, output_dir)\u001B[0m\n\u001B[0;32m    129\u001B[0m     segment_start_frame \u001B[38;5;241m=\u001B[39m frame_counter\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124müé¨ Started segment \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msegment_num\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtimedelta(seconds\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mint\u001B[39m(segment_start_frame\u001B[38;5;241m/\u001B[39morig_fps))\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 132\u001B[0m writer\u001B[38;5;241m.\u001B[39mwrite(processed_frame)\n\u001B[0;32m    133\u001B[0m frame_counter \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;66;03m# Progress reporting\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7,
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "from shutil import move\n",
    "\n",
    "# Configuration\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_SIZE = 640\n",
    "SEGMENT_DURATION = 600  # 10 minutes in seconds\n",
    "PROGRESS_INTERVAL = 45  # Seconds between updates\n",
    "BBOX_PADDING = 0.3\n",
    "FRAME_SKIP = 5\n",
    "MIN_SEGMENT_LENGTH = 300  # 5 minutes minimum duration\n",
    "MIN_CONFIDENCE = 0.4  # Adjusted confidence threshold\n",
    "HISTORY_BUFFER_SIZE = 900  # 25 second history (assuming 30 fps)\n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self):\n",
    "        self.model = YOLO('../models/bestRefereeDetection.pt').to(DEVICE)\n",
    "        self.model.fuse()\n",
    "        if DEVICE == 'cuda':\n",
    "            self.model.half()\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        self.class_id = self._get_referee_class_id()\n",
    "        self.detection_history = deque(maxlen=HISTORY_BUFFER_SIZE)\n",
    "        self.last_valid_bbox = None\n",
    "\n",
    "    def _get_referee_class_id(self):\n",
    "        return list(self.model.names.keys())[\n",
    "            list(self.model.names.values()).index('referee')\n",
    "        ]\n",
    "\n",
    "    def process_videos(self, input_dir='../data/input_videos', output_dir='../data/processed_videos', used_dir='../data/used_videos'):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(used_dir, exist_ok=True)\n",
    "\n",
    "        for video_file in [f for f in os.listdir(input_dir) if f.endswith('.mp4')]:\n",
    "            video_path = os.path.join(input_dir, video_file)\n",
    "            print(f\"\\nüöÄ Starting processing: {video_file}\")\n",
    "            try:\n",
    "                self._process_single_video(video_path, output_dir, used_dir)\n",
    "                print(f\"\\n‚úÖ Successfully processed: {video_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error processing {video_file}: {str(e)}\")\n",
    "\n",
    "    def _process_single_video(self, video_path, output_dir, used_dir):\n",
    "        duration = self._get_video_duration(video_path)\n",
    "        \n",
    "        if duration > 3600:\n",
    "            print(\"‚è≥ Splitting into 1-hour chunks...\")\n",
    "            chunks = self._split_into_hourly_chunks(video_path, output_dir)\n",
    "            print(f\"üì¶ Created {len(chunks)} temporary chunks\")\n",
    "            \n",
    "            for i, chunk in enumerate(chunks, 1):\n",
    "                print(f\"\\nüîß Processing chunk {i}/{len(chunks)}\")\n",
    "                self._process_video_chunk(chunk, output_dir)\n",
    "                os.remove(chunk)\n",
    "                print(f\"üßπ Cleaned temporary chunk {i}\")\n",
    "            \n",
    "            self._safe_move(video_path, used_dir)\n",
    "            print(f\"\\nüì¶ Moved original to: {used_dir}\")\n",
    "            return\n",
    "\n",
    "        print(\"üîç Processing single video chunk...\")\n",
    "        self._process_video_chunk(video_path, output_dir)\n",
    "        self._safe_move(video_path, used_dir)\n",
    "        print(f\"\\nüì¶ Moved original to: {used_dir}\")\n",
    "\n",
    "    def _process_video_chunk(self, video_path, output_dir):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        original_duration = total_frames / orig_fps\n",
    "        \n",
    "        base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        writer = None\n",
    "        segment_num = 1\n",
    "        segment_start_frame = 0\n",
    "        last_progress_update = time.time()\n",
    "        detections = 0\n",
    "        frame_counter = 0\n",
    "\n",
    "        try:\n",
    "            while frame_counter < total_frames:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                # Process detection every FRAME_SKIP frames\n",
    "                if frame_counter % FRAME_SKIP == 0:\n",
    "                    processed_frame, detected = self._process_frame(frame, self.last_valid_bbox)\n",
    "                    if detected:\n",
    "                        detections += 1\n",
    "                        self.last_valid_bbox = self._get_consensus_bbox(frame.shape)\n",
    "                \n",
    "                # Use last valid detection for all frames\n",
    "                if self.last_valid_bbox:\n",
    "                    x1, y1, x2, y2 = self.last_valid_bbox\n",
    "                    cropped = frame[y1:y2, x1:x2]\n",
    "                    processed_frame = cv2.resize(cropped, (MODEL_SIZE, MODEL_SIZE))\n",
    "                else:\n",
    "                    processed_frame = np.zeros((MODEL_SIZE, MODEL_SIZE, 3), dtype=np.uint8)\n",
    "\n",
    "                # Create new segment when needed\n",
    "                if writer is None or (frame_counter / orig_fps - segment_start_frame / orig_fps >= SEGMENT_DURATION):\n",
    "                    if writer:\n",
    "                        writer.release()\n",
    "                        print(f\"üíæ Saved segment {segment_num} ({timedelta(seconds=int(SEGMENT_DURATION))})\")\n",
    "                    segment_num += 1\n",
    "                    writer = self._create_writer(base_name, timestamp, output_dir, segment_num, orig_fps)\n",
    "                    segment_start_frame = frame_counter\n",
    "                    print(f\"üé¨ Started segment {segment_num} at {timedelta(seconds=int(segment_start_frame/orig_fps))}\")\n",
    "\n",
    "                writer.write(processed_frame)\n",
    "                frame_counter += 1\n",
    "\n",
    "                # Progress reporting\n",
    "                if time.time() - last_progress_update >= PROGRESS_INTERVAL:\n",
    "                    elapsed = frame_counter / orig_fps\n",
    "                    pct = (elapsed / original_duration) * 100\n",
    "                    print(f\"\\n‚è±Ô∏è Progress [{timedelta(seconds=int(elapsed))}] \"\n",
    "                          f\"{pct:.1f}% complete | \"\n",
    "                          f\"Segments: {segment_num} | \"\n",
    "                          f\"Recent detections: {detections}\")\n",
    "                    last_progress_update = time.time()\n",
    "                    detections = 0\n",
    "\n",
    "        finally:\n",
    "            if writer:\n",
    "                final_duration = (frame_counter - segment_start_frame) / orig_fps\n",
    "                if final_duration >= MIN_SEGMENT_LENGTH:\n",
    "                    writer.release()\n",
    "                    print(f\"üíæ Saved final segment {segment_num} ({timedelta(seconds=int(final_duration))})\")\n",
    "                else:\n",
    "                    writer.release()\n",
    "                    os.remove(writer.filename)\n",
    "                    print(f\"üßπ Removed short segment (<{MIN_SEGMENT_LENGTH}s)\")\n",
    "            cap.release()\n",
    "\n",
    "    def _process_frame(self, frame, last_valid):\n",
    "        original_frame = frame.copy()\n",
    "        h, w = frame.shape[:2]\n",
    "        \n",
    "        # Prepare tensor (BCHW format)\n",
    "        resized_frame = cv2.resize(frame, (MODEL_SIZE, MODEL_SIZE), interpolation=cv2.INTER_LINEAR)\n",
    "        tensor = torch.from_numpy(resized_frame).to(DEVICE)\n",
    "        tensor = tensor.permute(2, 0, 1)\n",
    "        tensor = tensor.half() if DEVICE == 'cuda' else tensor.float()\n",
    "        tensor = (tensor / 255.0).unsqueeze(0)\n",
    "    \n",
    "        # Run inference with suppressed outputs\n",
    "        with torch.no_grad():\n",
    "            results = self.model(tensor, verbose=False)[0]\n",
    "    \n",
    "        # Process results with padding\n",
    "        if len(results.boxes) > 0:\n",
    "            valid_detections = [box for box in results.boxes.data \n",
    "                              if box[4] >= MIN_CONFIDENCE \n",
    "                              and (box[2]-box[0])*(box[3]-box[1]) > 1000]\n",
    "            \n",
    "            if valid_detections:\n",
    "                best_detection = max(valid_detections, key=lambda x: x[4])\n",
    "                self.detection_history.append(best_detection[:4].cpu().numpy())\n",
    "        \n",
    "        consensus_bbox = self._get_consensus_bbox(frame.shape)\n",
    "        \n",
    "        if consensus_bbox is not None:\n",
    "            x1, y1, x2, y2 = consensus_bbox\n",
    "            cropped = original_frame[y1:y2, x1:x2]\n",
    "            processed_frame = cv2.resize(cropped, (MODEL_SIZE, MODEL_SIZE), interpolation=cv2.INTER_LINEAR)\n",
    "            return processed_frame, True\n",
    "        \n",
    "        return last_valid, False\n",
    "\n",
    "    def _get_consensus_bbox(self, shape):\n",
    "        if not self.detection_history:\n",
    "            return None\n",
    "        \n",
    "        # Calculate median of recent detections\n",
    "        median_bbox = np.median(self.detection_history, axis=0)\n",
    "        h, w = shape[:2]\n",
    "        \n",
    "        # Convert coordinates to original frame scale\n",
    "        x_scale = w / MODEL_SIZE\n",
    "        y_scale = h / MODEL_SIZE\n",
    "        \n",
    "        # Apply padding and boundary checks\n",
    "        bbox_width = (median_bbox[2] - median_bbox[0]) * x_scale\n",
    "        bbox_height = (median_bbox[3] - median_bbox[1]) * y_scale\n",
    "        pad_x = int(bbox_width * BBOX_PADDING)\n",
    "        pad_y = int(bbox_height * BBOX_PADDING)\n",
    "        \n",
    "        return (\n",
    "            max(0, int(median_bbox[0] * x_scale - pad_x)),\n",
    "            max(0, int(median_bbox[1] * y_scale - pad_y)),\n",
    "            min(w, int(median_bbox[2] * x_scale + pad_x)),\n",
    "            min(h, int(median_bbox[3] * y_scale + pad_y))\n",
    "        )\n",
    "\n",
    "\n",
    "    def _create_writer(self, base_name, timestamp, output_dir, segment_num, fps):\n",
    "        output_path = os.path.join(\n",
    "            output_dir,\n",
    "            f\"{base_name}_{timestamp}_part{segment_num:03d}.mp4\"\n",
    "        )\n",
    "        return cv2.VideoWriter(\n",
    "            output_path,\n",
    "            cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "            fps,\n",
    "            (MODEL_SIZE, MODEL_SIZE)\n",
    "        )\n",
    "\n",
    "    def _split_into_hourly_chunks(self, video_path, output_dir):\n",
    "        base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_pattern = os.path.join(output_dir, f\"{base_name}_{timestamp}_temp_part%03d.mp4\")\n",
    "        \n",
    "        try:\n",
    "            subprocess.run([\n",
    "                'ffmpeg', '-i', video_path,\n",
    "                '-c', 'copy',\n",
    "                '-map', '0',\n",
    "                '-segment_time', '01:00:00',\n",
    "                '-f', 'segment',\n",
    "                '-reset_timestamps', '1',\n",
    "                '-loglevel', 'error',\n",
    "                output_pattern\n",
    "            ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå FFmpeg error: {e.stderr.decode()}\")\n",
    "            raise\n",
    "\n",
    "        return sorted([os.path.join(output_dir, f) for f in os.listdir(output_dir) \n",
    "                      if f.startswith(f\"{base_name}_{timestamp}_temp\")])\n",
    "\n",
    "    def _get_video_duration(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        cap.release()\n",
    "        return frames / fps if fps else 0\n",
    "\n",
    "    def _safe_move(self, src, dest_dir):\n",
    "        dest = os.path.join(dest_dir, os.path.basename(src))\n",
    "        for attempt in range(5):\n",
    "            try:\n",
    "                if os.path.exists(dest):\n",
    "                    os.remove(dest)\n",
    "                move(src, dest)\n",
    "                print(f\"üì§ Moved {os.path.basename(src)} successfully\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                if attempt == 4:\n",
    "                    print(f\"‚ùå Failed to move {os.path.basename(src)}: {str(e)}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = VideoProcessor()\n",
    "    print(\"üöÄ Starting video processing pipeline...\")\n",
    "    processor.process_videos()\n",
    "    print(\"\\nüéâ All processing completed successfully!\")\n"
   ],
   "id": "fbc121e30a2defb3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
